{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HJ8_2ipZT2L"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojrhtme4ZT2M"
      },
      "source": [
        "Our goal is to look at transcripts of various comedians and note their similarities and differences. Specifically, I'd like to know if Ali Wong's comedy style is different than other comedians, since she's the comedian that got me interested in stand up comedy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YXGH25YZT2E"
      },
      "source": [
        "# Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZsqoIT1ZT2K"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15oF36EfZT2L"
      },
      "source": [
        "This notebook goes through a necessary step of any data science project - data cleaning. Data cleaning is a time consuming and unenjoyable task, yet it's a very important one. Keep in mind, \"garbage in, garbage out\". Feeding dirty data into a model will give us results that are meaningless.\n",
        "\n",
        "Specifically, we'll be walking through:\n",
        "\n",
        "1. **Getting the data - **in this case, we'll be scraping data from a website\n",
        "2. **Cleaning the data - **we will walk through popular text pre-processing techniques\n",
        "3. **Organizing the data - **we will organize the cleaned data into a way that is easy to input into other algorithms\n",
        "\n",
        "The output of this notebook will be clean, organized data in two standard text formats:\n",
        "\n",
        "1. **Corpus** - a collection of text\n",
        "2. **Document-Term Matrix** - word counts in matrix format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEVwpfPtZT2M"
      },
      "source": [
        "## Getting The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1iVwwZwZT2N"
      },
      "source": [
        "Luckily, there are wonderful people online that keep track of stand up routine transcripts. [Scraps From The Loft](http://scrapsfromtheloft.com) makes them available for non-profit and educational purposes.\n",
        "\n",
        "To decide which comedians to look into, I went on IMDB and looked specifically at comedy specials that were released in the past 5 years. To narrow it down further, I looked only at those with greater than a 7.5/10 rating and more than 2000 votes. If a comedian had multiple specials that fit those requirements, I would pick the most highly rated one. I ended up with a dozen comedy specials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "EM83RBo6ZT2O"
      },
      "outputs": [],
      "source": [
        "# Web scraping, pickle imports\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "\n",
        "# Scrapes transcript data from scrapsfromtheloft.com\n",
        "def url_to_transcript(url):\n",
        "    '''Returns transcript data specifically from scrapsfromtheloft.com.'''\n",
        "    page = requests.get(url).text\n",
        "    soup = BeautifulSoup(page, \"lxml\")\n",
        "    text = [p.text for p in soup.find(class_=\"elementor-widget-theme-post-content\").find_all('p')]\n",
        "    print(url)\n",
        "    return text\n",
        "\n",
        "# URLs of transcripts in scope\n",
        "urls = ['http://scrapsfromtheloft.com/2017/05/06/louis-ck-oh-my-god-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/04/11/dave-chappelle-age-spin-2017-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2018/03/15/ricky-gervais-humanity-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/05/24/bill-burr-im-sorry-feel-way-2014-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/04/21/jim-jefferies-bare-2014-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/02/john-mulaney-comeback-kid-2015-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/09/19/ali-wong-baby-cobra-2016-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/03/anthony-jeselnik-thoughts-prayers-2015-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2018/03/03/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript/',\n",
        "        'http://scrapsfromtheloft.com/2017/08/19/joe-rogan-triggered-2016-full-transcript/']\n",
        "\n",
        "# Comedian names\n",
        "comedians = ['louis', 'dave', 'ricky', 'bill', 'jim', 'john', 'ali', 'anthony', 'mike', 'joe']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1JXqIMNZT2P"
      },
      "outputs": [],
      "source": [
        "# Actually request transcripts (takes a few minutes to run)\n",
        "transcripts = [url_to_transcript(u) for u in urls]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcripts[0]"
      ],
      "metadata": {
        "id": "uE-1-Ip0umSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1WmI6UnTZT2Q"
      },
      "outputs": [],
      "source": [
        "# Pickle files for later use\n",
        "\n",
        "# Make a new directory to hold the text files\n",
        "!mkdir transcripts\n",
        "\n",
        "for i, c in enumerate(comedians):\n",
        "     with open(\"transcripts/\" + c + \".txt\", \"wb\") as file:\n",
        "        pickle.dump(transcripts[i], file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xNiSqGskZT2R"
      },
      "outputs": [],
      "source": [
        "# Load pickled files\n",
        "data = {}\n",
        "for i, c in enumerate(comedians):\n",
        "    with open(\"transcripts/\" + c + \".txt\", \"rb\") as file:\n",
        "        data[c] = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pickle files for later use\n",
        "\n",
        "# Make a new directory to hold the text files\n",
        "#!mkdir transcripts\n",
        "\n",
        "#for i, c in enumerate(comedians):\n",
        "#     with open(\"transcripts/\" + c + \".txt\", \"w\") as file:\n",
        "#       file.write(''.join(transcripts[i]))"
      ],
      "metadata": {
        "id": "_eRItjmhP0j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pickled files\n",
        "#data = {}\n",
        "#for i, c in enumerate(comedians):\n",
        "#    with open(\"transcripts/\" + c + \".txt\", \"r\") as file:\n",
        "#        data[c] = file.read()"
      ],
      "metadata": {
        "id": "vpSvN0G9P1bU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlG-yk1WZT2R"
      },
      "outputs": [],
      "source": [
        "# Double check to make sure data has been loaded properly\n",
        "data.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-teIfQogZT2R"
      },
      "outputs": [],
      "source": [
        "# More checks\n",
        "data['louis'][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn28Fb_iZT2S"
      },
      "source": [
        "## Cleaning The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8H7Ur48ZT2S"
      },
      "source": [
        "When dealing with numerical data, data cleaning often involves removing null values and duplicate data, dealing with outliers, etc. With text data, there are some common data cleaning techniques, which are also known as text pre-processing techniques.\n",
        "\n",
        "With text data, this cleaning process can go on forever. There's always an exception to every cleaning step. So, we're going to follow the MVP (minimum viable product) approach - start simple and iterate. Here are a bunch of things you can do to clean your data. We're going to execute just the common cleaning steps here and the rest can be done at a later point to improve our results.\n",
        "\n",
        "**Common data cleaning steps on all text:**\n",
        "* Make text all lower case\n",
        "* Remove punctuation\n",
        "* Remove numerical values\n",
        "* Remove common non-sensical text (/n)\n",
        "* Tokenize text\n",
        "* Remove stop words\n",
        "\n",
        "**More data cleaning steps after tokenization:**\n",
        "* Stemming / lemmatization\n",
        "* Parts of speech tagging\n",
        "* Create bi-grams or tri-grams\n",
        "* Deal with typos\n",
        "* And more..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuK8_95qZT2S"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at our data again\n",
        "next(iter(data.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5TD6eFQZT2T"
      },
      "outputs": [],
      "source": [
        "# Notice that our dictionary is currently in key: comedian, value: list of text format\n",
        "next(iter(data.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rA14EVpNZT2T"
      },
      "outputs": [],
      "source": [
        "# We are going to change this to key: comedian, value: string format\n",
        "def combine_text(list_of_text):\n",
        "    '''Takes a list of text and combines them into one large chunk of text.'''\n",
        "    combined_text = ' '.join(list_of_text)\n",
        "    return combined_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sKJba3xxZT2T"
      },
      "outputs": [],
      "source": [
        "# Combine it!\n",
        "data_combined = {key: [combine_text(value)] for (key, value) in data.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vQHYhPUZT2U"
      },
      "outputs": [],
      "source": [
        "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
        "import pandas as pd\n",
        "pd.set_option('max_colwidth',150)\n",
        "\n",
        "data_df = pd.DataFrame.from_dict(data_combined).transpose()\n",
        "data_df.columns = ['transcript']\n",
        "data_df = data_df.sort_index()\n",
        "data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_mbS3LNZT2U"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the transcript for Ali Wong\n",
        "data_df.transcript.loc['ali']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rn0EQ1WKZT2U"
      },
      "outputs": [],
      "source": [
        "# Apply a first round of text cleaning techniques\n",
        "import re\n",
        "import string\n",
        "\n",
        "def clean_text_round1(text):\n",
        "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text\n",
        "\n",
        "round1 = lambda x: clean_text_round1(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_ZRj-vZZT2U"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the updated text\n",
        "data_clean = pd.DataFrame(data_df.transcript.apply(round1))\n",
        "data_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_V12wqsnZT2V"
      },
      "outputs": [],
      "source": [
        "# Apply a second round of cleaning\n",
        "def clean_text_round2(text):\n",
        "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    return text\n",
        "\n",
        "round2 = lambda x: clean_text_round2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsJ1JMvrZT2V"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the updated text\n",
        "data_clean = pd.DataFrame(data_clean.transcript.apply(round2))\n",
        "data_clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNLcu3u5ZT2V"
      },
      "source": [
        "**NOTE:** This data cleaning aka text pre-processing step could go on for a while, but we are going to stop for now. After going through some analysis techniques, if you see that the results don't make sense or could be improved, you can come back and make more edits such as:\n",
        "* Mark 'cheering' and 'cheer' as the same word (stemming / lemmatization)\n",
        "* Combine 'thank you' into one term (bi-grams)\n",
        "* And a lot more..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eULRFq7qZT2V"
      },
      "source": [
        "## Organizing The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9gc4DOAZT2V"
      },
      "source": [
        "I mentioned earlier that the output of this notebook will be clean, organized data in two standard text formats:\n",
        "1. **Corpus - **a collection of text\n",
        "2. **Document-Term Matrix - **word counts in matrix format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndI18zZmZT2W"
      },
      "source": [
        "### Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da0b_nfcZT2W"
      },
      "source": [
        "We already created a corpus in an earlier step. The definition of a corpus is a collection of texts, and they are all put together neatly in a pandas dataframe here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1N2ibKnZT2W"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at our dataframe\n",
        "data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIQhjdKEZT2W"
      },
      "outputs": [],
      "source": [
        "# Let's add the comedians' full names as well\n",
        "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Dave Chappelle',\n",
        "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
        "\n",
        "data_df['full_name'] = full_names\n",
        "data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sDDDQo5wZT2W"
      },
      "outputs": [],
      "source": [
        "# Let's pickle it for later use\n",
        "data_df.to_pickle(\"corpus.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QXOwWfPZT2W"
      },
      "source": [
        "### Document-Term Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXtMEKW5ZT2X"
      },
      "source": [
        "For many of the techniques we'll be using in future notebooks, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
        "\n",
        "In addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIHvjnE3ZT2X"
      },
      "outputs": [],
      "source": [
        "# We are going to create a document-term matrix using CountVectorizer, and exclude common English stop words\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "data_cv = cv.fit_transform(data_clean.transcript)\n",
        "data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_dtm.index = data_clean.index\n",
        "data_dtm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "r2JzDkUlZT2X"
      },
      "outputs": [],
      "source": [
        "# Let's pickle it for later use\n",
        "data_dtm.to_pickle(\"dtm.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KXGXnyaVZT2X"
      },
      "outputs": [],
      "source": [
        "# Let's also pickle the cleaned data (before we put it in document-term matrix format) and the CountVectorizer object\n",
        "data_clean.to_pickle('data_clean.pkl')\n",
        "pickle.dump(cv, open(\"cv.pkl\", \"wb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "gfqXuQj0Zj7d"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPmHYA_WZj7f"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrT8vfaCZj7g"
      },
      "source": [
        "After the data cleaning step where we put our data into a few standard formats, the next step is to take a look at the data and see if what we're looking at makes sense. Before applying any fancy algorithms, it's always important to explore the data first.\n",
        "\n",
        "When working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. We are going to look at the following for each comedian:\n",
        "\n",
        "1. **Most common words** - find these and create word clouds\n",
        "2. **Size of vocabulary** - look number of unique words and also how quickly someone speaks\n",
        "3. **Amount of profanity** - most common terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3egMldMvZj7g"
      },
      "source": [
        "## Most Common Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RllWHKALZj7g"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMamqUXtZj7h"
      },
      "outputs": [],
      "source": [
        "# Read in the document-term matrix\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_pickle('dtm.pkl')\n",
        "data = data.transpose()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuW5jyXyZj7h"
      },
      "outputs": [],
      "source": [
        "# Find the top 30 words said by each comedian\n",
        "top_dict = {}\n",
        "for c in data.columns:\n",
        "    top = data[c].sort_values(ascending=False).head(30)\n",
        "    top_dict[c]= list(zip(top.index, top.values))\n",
        "\n",
        "top_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdaauKYbZj7i"
      },
      "outputs": [],
      "source": [
        "# Print the top 15 words said by each comedian\n",
        "for comedian, top_words in top_dict.items():\n",
        "    print(comedian)\n",
        "    print(', '.join([word for word, count in top_words[0:14]]))\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkuMR3xNZj7i"
      },
      "source": [
        "**NOTE:** At this point, we could go on and create word clouds. However, by looking at these top words, you can see that some of them have very little meaning and could be added to a stop words list, so let's do just that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy88fT7NZj7j"
      },
      "outputs": [],
      "source": [
        "# Look at the most common top words --> add them to the stop word list\n",
        "from collections import Counter\n",
        "\n",
        "# Let's first pull out the top 30 words for each comedian\n",
        "words = []\n",
        "for comedian in data.columns:\n",
        "    top = [word for (word, count) in top_dict[comedian]]\n",
        "    for t in top:\n",
        "        words.append(t)\n",
        "        \n",
        "words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VRflN6GZj7j"
      },
      "outputs": [],
      "source": [
        "# Let's aggregate this list and identify the most common words along with how many routines they occur in\n",
        "Counter(words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GueS7EFgZj7j"
      },
      "outputs": [],
      "source": [
        "# If more than half of the comedians have it as a top word, exclude it from the list\n",
        "add_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\n",
        "add_stop_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jUIifaMNZj7j"
      },
      "outputs": [],
      "source": [
        "# Let's update our document-term matrix with the new list of stop words\n",
        "from sklearn.feature_extraction import text \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Read in cleaned data\n",
        "data_clean = pd.read_pickle('data_clean.pkl')\n",
        "\n",
        "# Add new stop words\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# Recreate document-term matrix\n",
        "cv = CountVectorizer(stop_words=stop_words)\n",
        "data_cv = cv.fit_transform(data_clean.transcript)\n",
        "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
        "data_stop.index = data_clean.index\n",
        "\n",
        "# Pickle it for later use\n",
        "import pickle\n",
        "pickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\n",
        "data_stop.to_pickle(\"dtm_stop.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dpPcZiekZj7k"
      },
      "outputs": [],
      "source": [
        "# Let's make some word clouds!\n",
        "# Terminal / Anaconda Prompt: conda install -c conda-forge wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "wc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n",
        "               max_font_size=150, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr9PMrIpZj7k"
      },
      "outputs": [],
      "source": [
        "# Reset the output dimensions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [16, 6]\n",
        "\n",
        "full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Dave Chappelle',\n",
        "              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n",
        "\n",
        "# Create subplots for each comedian\n",
        "for index, comedian in enumerate(data.columns):\n",
        "    wc.generate(data_clean.transcript[comedian])\n",
        "    \n",
        "    plt.subplot(3, 4, index+1)\n",
        "    plt.imshow(wc, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(full_names[index])\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFzOKtmxZj7k"
      },
      "source": [
        "### Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yG6TWIgfZj7l"
      },
      "source": [
        "* Ali Wong says the s-word a lot and talks about her husband. I guess that's funny to me.\n",
        "* A lot of people use the F-word. Let's dig into that later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvzA89gDZj7l"
      },
      "source": [
        "## Number of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN_WUWI8Zj7l"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M76CHtoeZj7l"
      },
      "outputs": [],
      "source": [
        "# Find the number of unique words that each comedian uses\n",
        "\n",
        "# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\n",
        "unique_list = []\n",
        "for comedian in data.columns:\n",
        "    uniques = data[comedian].to_numpy().nonzero()[0].size\n",
        "    unique_list.append(uniques)\n",
        "\n",
        "# Create a new dataframe that contains this unique word count\n",
        "data_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\n",
        "data_unique_sort = data_words.sort_values(by='unique_words')\n",
        "data_unique_sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lytKpn9LZj7m"
      },
      "outputs": [],
      "source": [
        "# Calculate the words per minute of each comedian\n",
        "\n",
        "# Find the total number of words that a comedian uses\n",
        "total_list = []\n",
        "for comedian in data.columns:\n",
        "    totals = sum(data[comedian])\n",
        "    total_list.append(totals)\n",
        "    \n",
        "# Comedy special run times from IMDB, in minutes\n",
        "run_times = [60, 59, 80, 67, 73, 77, 62, 58, 76, 79]\n",
        "\n",
        "# Let's add some columns to our dataframe\n",
        "data_words['total_words'] = total_list\n",
        "data_words['run_times'] = run_times\n",
        "data_words['words_per_minute'] = data_words['total_words'] / data_words['run_times']\n",
        "\n",
        "# Sort the dataframe by words per minute to see who talks the slowest and fastest\n",
        "data_wpm_sort = data_words.sort_values(by='words_per_minute')\n",
        "data_wpm_sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SGz-U3ZyZj7m"
      },
      "outputs": [],
      "source": [
        "# Let's plot our findings\n",
        "import numpy as np\n",
        "\n",
        "y_pos = np.arange(len(data_words))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(y_pos, data_unique_sort.unique_words, align='center')\n",
        "plt.yticks(y_pos, data_unique_sort.comedian)\n",
        "plt.title('Number of Unique Words', fontsize=20)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.barh(y_pos, data_wpm_sort.words_per_minute, align='center')\n",
        "plt.yticks(y_pos, data_wpm_sort.comedian)\n",
        "plt.title('Number of Words Per Minute', fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybp47blQZj7m"
      },
      "source": [
        "### Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8piRBnmlZj7m"
      },
      "source": [
        "* **Vocabulary**\n",
        "   * Ricky Gervais (British comedy) and Bill Burr (podcast host) use a lot of words in their comedy\n",
        "   * Louis C.K. (self-depricating comedy) and Anthony Jeselnik (dark humor) have a smaller vocabulary\n",
        "\n",
        "\n",
        "* **Talking Speed**\n",
        "   * Joe Rogan (blue comedy) and Bill Burr (podcast host) talk fast\n",
        "   * Bo Burnham (musical comedy) and Anthony Jeselnik (dark humor) talk slow\n",
        "   \n",
        "Ali Wong is somewhere in the middle in both cases. Nothing too interesting here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScKZKrePZj7n"
      },
      "source": [
        "## Amount of Profanity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr-naIX_Zj7n"
      },
      "source": [
        "### Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZ09cL4uZj7n"
      },
      "outputs": [],
      "source": [
        "# Earlier I said we'd revisit profanity. Let's take a look at the most common words again.\n",
        "Counter(words).most_common()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxFYPqH5Zj7n"
      },
      "outputs": [],
      "source": [
        "# Let's isolate just these bad words\n",
        "data_bad_words = data.transpose()[['fucking', 'fuck', 'shit']]\n",
        "data_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)\n",
        "data_profanity.columns = ['f_word', 's_word']\n",
        "data_profanity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DKn1GyJTZj7n"
      },
      "outputs": [],
      "source": [
        "# Let's create a scatter plot of our findings\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "\n",
        "for i, comedian in enumerate(data_profanity.index):\n",
        "    x = data_profanity.f_word.loc[comedian]\n",
        "    y = data_profanity.s_word.loc[comedian]\n",
        "    plt.scatter(x, y, color='blue')\n",
        "    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n",
        "    plt.xlim(-5, 155) \n",
        "    \n",
        "plt.title('Number of Bad Words Used in Routine', fontsize=20)\n",
        "plt.xlabel('Number of F Bombs', fontsize=15)\n",
        "plt.ylabel('Number of S Words', fontsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlXg8jlaZj7o"
      },
      "source": [
        "### Findings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MvuyW4iZj7o"
      },
      "source": [
        "* **Averaging 2 F-Bombs Per Minute!** - I don't like too much swearing, especially the f-word, which is probably why I've never heard of Bill Bur, Joe Rogan and Jim Jefferies.\n",
        "* **Clean Humor** - It looks like profanity might be a good predictor of the type of comedy I like. Besides Ali Wong, my two other favorite comedians in this group are John Mulaney and Mike Birbiglia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "0ky1N1qeZj7o"
      },
      "source": [
        "## Side Note"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8I14-t9Zj7o"
      },
      "source": [
        "What was our goal for the EDA portion of our journey? **To be able to take an initial look at our data and see if the results of some basic analysis made sense.**\n",
        "\n",
        "My conclusion - yes, it does, for a first pass. There are definitely some things that could be better cleaned up, such as adding more stop words or including bi-grams. But we can save that for another day. The results, especially the profanity findings, are interesting and make general sense, so we're going to move on.\n",
        "\n",
        "As a reminder, the data science process is an interative one. It's better to see some non-perfect but acceptable results to help you quickly decide whether your project is a dud or not, instead of having analysis paralysis and never delivering anything.\n",
        "\n",
        "**Alice's data science (and life) motto: Let go of perfectionism!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SChp3ELiZ0cM"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpffYravZ0cO"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEKTGMQsZ0cP"
      },
      "source": [
        "So far, all of the analysis we've done has been pretty generic - looking at counts, creating scatter plots, etc. These techniques could be applied to numeric data as well.\n",
        "\n",
        "When it comes to text data, there are a few popular techniques that we'll be going through in the next few notebooks, starting with sentiment analysis. A few key points to remember with sentiment analysis.\n",
        "\n",
        "1. **TextBlob Module:** Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels.\n",
        "2. **Sentiment Labels:** Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these.\n",
        "   * **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n",
        "   * **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n",
        "\n",
        "For more info on how TextBlob coded up its [sentiment function](https://planspace.org/20150607-textblob_sentiment/).\n",
        "\n",
        "Let's take a look at the sentiment of the various transcripts, both overall and throughout the comedy routine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4We98DLFZ0cQ"
      },
      "source": [
        "## Sentiment of Routine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnM0Yo0NZ0cQ"
      },
      "outputs": [],
      "source": [
        "# We'll start by reading in the corpus, which preserves word order\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_pickle('corpus.pkl')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6szxYPj8Z0cR"
      },
      "outputs": [],
      "source": [
        "# Create quick lambda functions to find the polarity and subjectivity of each routine\n",
        "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "pol = lambda x: TextBlob(x).sentiment.polarity\n",
        "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
        "\n",
        "data['polarity'] = data['transcript'].apply(pol)\n",
        "data['subjectivity'] = data['transcript'].apply(sub)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSH56ZeGZ0cS"
      },
      "outputs": [],
      "source": [
        "# Let's plot the results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [10, 8]\n",
        "\n",
        "for index, comedian in enumerate(data.index):\n",
        "    x = data.polarity.loc[comedian]\n",
        "    y = data.subjectivity.loc[comedian]\n",
        "    plt.scatter(x, y, color='blue')\n",
        "    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)\n",
        "    plt.xlim(-.01, .12) \n",
        "    \n",
        "plt.title('Sentiment Analysis', fontsize=20)\n",
        "plt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\n",
        "plt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjHj1LocZ0cS"
      },
      "source": [
        "## Sentiment of Routine Over Time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hio2KoRTZ0cT"
      },
      "source": [
        "Instead of looking at the overall sentiment, let's see if there's anything interesting about the sentiment over time throughout each routine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-YXAy-olZ0cT"
      },
      "outputs": [],
      "source": [
        "# Split each routine into 10 parts\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def split_text(text, n=10):\n",
        "    '''Takes in a string of text and splits into n equal parts, with a default of 10 equal parts.'''\n",
        "\n",
        "    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n",
        "    length = len(text)\n",
        "    size = math.floor(length / n)\n",
        "    start = np.arange(0, length, size)\n",
        "    \n",
        "    # Pull out equally sized pieces of text and put it into a list\n",
        "    split_list = []\n",
        "    for piece in range(n):\n",
        "        split_list.append(text[start[piece]:start[piece]+size])\n",
        "    return split_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDvJnvDiZ0cU"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at our data again\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owivbhoRZ0cU"
      },
      "outputs": [],
      "source": [
        "# Let's create a list to hold all of the pieces of text\n",
        "list_pieces = []\n",
        "for t in data.transcript:\n",
        "    split = split_text(t)\n",
        "    list_pieces.append(split)\n",
        "    \n",
        "list_pieces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJKOjdp5Z0cV"
      },
      "outputs": [],
      "source": [
        "# The list has 10 elements, one for each transcript\n",
        "len(list_pieces)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDZyAUvZZ0cV"
      },
      "outputs": [],
      "source": [
        "# Each transcript has been split into 10 pieces of text\n",
        "len(list_pieces[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9t6OtWteZ0cV"
      },
      "outputs": [],
      "source": [
        "# Calculate the polarity for each piece of text\n",
        "\n",
        "polarity_transcript = []\n",
        "for lp in list_pieces:\n",
        "    polarity_piece = []\n",
        "    for p in lp:\n",
        "        polarity_piece.append(TextBlob(p).sentiment.polarity)\n",
        "    polarity_transcript.append(polarity_piece)\n",
        "    \n",
        "polarity_transcript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Uo0Og_B_Z0cW"
      },
      "outputs": [],
      "source": [
        "# Show the plot for one comedian\n",
        "plt.plot(polarity_transcript[0])\n",
        "plt.title(data['full_name'].index[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YWfbneRZ0cW"
      },
      "outputs": [],
      "source": [
        "# Show the plot for all comedians\n",
        "plt.rcParams['figure.figsize'] = [16, 12]\n",
        "\n",
        "for index, comedian in enumerate(data.index):    \n",
        "    plt.subplot(3, 4, index+1)\n",
        "    plt.plot(polarity_transcript[index])\n",
        "    plt.plot(np.arange(0,10), np.zeros(10))\n",
        "    plt.title(data['full_name'][index])\n",
        "    plt.ylim(ymin=-.2, ymax=.3)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "h9fmpz7oZ0cW"
      },
      "source": [
        "Ali Wong stays generally positive throughout her routine. Similar comedians are Louis C.K. and Mike Birbiglia.\n",
        "\n",
        "On the other hand, you have some pretty different patterns here like Bo Burnham who gets happier as time passes and Dave Chappelle who has some pretty down moments in his routine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC64tmxXaDIv"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KQJsRH8aDIx"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7KH69q8aDIy"
      },
      "source": [
        "Another popular text analysis technique is called topic modeling. The ultimate goal of topic modeling is to find various topics that are present in your corpus. Each document in the corpus will be made up of at least one topic, if not multiple topics.\n",
        "\n",
        "In this notebook, we will be covering the steps on how to do **Latent Dirichlet Allocation (LDA)**, which is one of many topic modeling techniques. It was specifically designed for text data.\n",
        "\n",
        "To use a topic modeling technique, you need to provide (1) a document-term matrix and (2) the number of topics you would like the algorithm to pick up.\n",
        "\n",
        "Once the topic modeling technique is applied, your job as a human is to interpret the results and see if the mix of words in each topic make sense. If they don't make sense, you can try changing up the number of topics, the terms in the document-term matrix, model parameters, or even try a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvJSVVUaDIy"
      },
      "source": [
        "## Topic Modeling - Attempt #1 (All Text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bu_2zhXaDIz"
      },
      "outputs": [],
      "source": [
        "# Let's read in our document-term matrix\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "data = pd.read_pickle('dtm_stop.pkl')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6Xz1fFH3aDI0"
      },
      "outputs": [],
      "source": [
        "# Import the necessary modules for LDA with gensim\n",
        "# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\n",
        "from gensim import matutils, models\n",
        "import scipy.sparse\n",
        "\n",
        "# import logging\n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo2QEz05aDI1"
      },
      "outputs": [],
      "source": [
        "# One of the required inputs is a term-document matrix\n",
        "tdm = data.transpose()\n",
        "tdm.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kjR_YhyCaDI1"
      },
      "outputs": [],
      "source": [
        "# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n",
        "sparse_counts = scipy.sparse.csr_matrix(tdm)\n",
        "corpus = matutils.Sparse2Corpus(sparse_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XBu5FL--aDI2"
      },
      "outputs": [],
      "source": [
        "# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n",
        "cv = pickle.load(open(\"cv_stop.pkl\", \"rb\"))\n",
        "id2word = dict((v, k) for k, v in cv.vocabulary_.items())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoGQZWfCaDI2"
      },
      "source": [
        "Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes. Let's start the number of topics at 2, see if the results make sense, and increase the number from there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYFQjRNWaDI2"
      },
      "outputs": [],
      "source": [
        "# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n",
        "# we need to specify two other parameters as well - the number of topics and the number of passes\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRnuys5uaDI3"
      },
      "outputs": [],
      "source": [
        "# LDA for num_topics = 3\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=10)\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4zKXCpraDI3"
      },
      "outputs": [],
      "source": [
        "# LDA for num_topics = 4\n",
        "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=10)\n",
        "lda.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh2ejVflaDI4"
      },
      "source": [
        "These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJ-0QI81aDI4"
      },
      "source": [
        "## Topic Modeling - Attempt #2 (Nouns Only)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZD6Th-AaDI4"
      },
      "source": [
        "One popular trick is to look only at terms that are from one part of speech (only nouns, only adjectives, etc.). Check out the UPenn tag set: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w7kI10YTaDI4"
      },
      "outputs": [],
      "source": [
        "# Let's create a function to pull out nouns from a string of text\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "def nouns(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n",
        "    is_noun = lambda pos: pos[:2] == 'NN'\n",
        "    tokenized = word_tokenize(text)\n",
        "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n",
        "    return ' '.join(all_nouns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCDuNIyAaDI5"
      },
      "outputs": [],
      "source": [
        "# Read in the cleaned data, before the CountVectorizer step\n",
        "data_clean = pd.read_pickle('data_clean.pkl')\n",
        "data_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRfZpYuuaDI5"
      },
      "outputs": [],
      "source": [
        "# Apply the nouns function to the transcripts to filter only on nouns\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "data_nouns = pd.DataFrame(data_clean.transcript.apply(nouns))\n",
        "data_nouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APJCA-YuaDI5"
      },
      "outputs": [],
      "source": [
        "# Create a new document-term matrix using only nouns\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Re-add the additional stop words since we are recreating the document-term matrix\n",
        "add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n",
        "                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n",
        "stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n",
        "\n",
        "# Recreate a document-term matrix with only nouns\n",
        "cvn = CountVectorizer(stop_words=stop_words)\n",
        "data_cvn = cvn.fit_transform(data_nouns.transcript)\n",
        "data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n",
        "data_dtmn.index = data_nouns.index\n",
        "data_dtmn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Phi6sW_9aDI6"
      },
      "outputs": [],
      "source": [
        "# Create the gensim corpus\n",
        "corpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "id2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_VqSBkpaDI6"
      },
      "outputs": [],
      "source": [
        "# Let's start with 2 topics\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT4lWxz4aDI6"
      },
      "outputs": [],
      "source": [
        "# Let's try topics = 3\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=3, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12A_dzU6aDI6"
      },
      "outputs": [],
      "source": [
        "# Let's try 4 topics\n",
        "ldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\n",
        "ldan.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN8byXxlaDI7"
      },
      "source": [
        "## Topic Modeling - Attempt #3 (Nouns and Adjectives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c9jGdNoTaDI7"
      },
      "outputs": [],
      "source": [
        "# Let's create a function to pull out nouns from a string of text\n",
        "def nouns_adj(text):\n",
        "    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n",
        "    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n",
        "    tokenized = word_tokenize(text)\n",
        "    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n",
        "    return ' '.join(nouns_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oerhy1rZaDI7"
      },
      "outputs": [],
      "source": [
        "# Apply the nouns function to the transcripts to filter only on nouns\n",
        "data_nouns_adj = pd.DataFrame(data_clean.transcript.apply(nouns_adj))\n",
        "data_nouns_adj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyWLxb2eaDI7"
      },
      "outputs": [],
      "source": [
        "# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n",
        "cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n",
        "data_cvna = cvna.fit_transform(data_nouns_adj.transcript)\n",
        "data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n",
        "data_dtmna.index = data_nouns_adj.index\n",
        "data_dtmna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PxFTpcJLaDI7"
      },
      "outputs": [],
      "source": [
        "# Create the gensim corpus\n",
        "corpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n",
        "\n",
        "# Create the vocabulary dictionary\n",
        "id2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV9tQOkoaDI8"
      },
      "outputs": [],
      "source": [
        "# Let's start with 2 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=2, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37vsEIwtaDI8"
      },
      "outputs": [],
      "source": [
        "# Let's try 3 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge7VmjWQaDI8"
      },
      "outputs": [],
      "source": [
        "# Let's try 4 topics\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\n",
        "ldana.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6ASQE3XaDI8"
      },
      "source": [
        "## Identify Topics in Each Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT9nfBt-aDI8"
      },
      "source": [
        "Out of the 9 topic models we looked at, the nouns and adjectives, 4 topic one made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNkNqmjTaDI8"
      },
      "outputs": [],
      "source": [
        "# Our final LDA model (for now)\n",
        "ldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=80)\n",
        "ldana.print_topics()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0UhGEziaDI9"
      },
      "source": [
        "These four topics look pretty decent. Let's settle on these for now.\n",
        "* Topic 0: mom, parents\n",
        "* Topic 1: husband, wife\n",
        "* Topic 2: guns\n",
        "* Topic 3: profanity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xyBx2hOaDI9"
      },
      "outputs": [],
      "source": [
        "# Let's take a look at which topics each transcript contains\n",
        "corpus_transformed = ldana[corpusna]\n",
        "list(zip([a for [(a,b)] in corpus_transformed], data_dtmna.index))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "8pMfT0dcaDI9"
      },
      "source": [
        "For a first pass of LDA, these kind of make sense to me, so we'll call it a day for now.\n",
        "* Topic 0: mom, parents [Anthony, Hasan, Louis, Ricky]\n",
        "* Topic 1: husband, wife [Ali, John, Mike]\n",
        "* Topic 2: guns [Bill, Bo, Jim]\n",
        "* Topic 3: profanity [Dave, Joe]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUMkjOMXaWPH"
      },
      "source": [
        "# Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InDaXiUXaWPJ"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQm3_LjPaWPJ"
      },
      "source": [
        "Markov chains can be used for very basic text generation. Think about every word in a corpus as a state. We can make a simple assumption that the next word is only dependent on the previous word - which is the basic assumption of a Markov chain.\n",
        "\n",
        "Markov chains don't generate text as well as deep learning, but it's a good (and fun!) start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRjVo6UJaWPK"
      },
      "source": [
        "## Select Text to Imitate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLvlHiLPaWPK"
      },
      "source": [
        "In this notebook, we're specifically going to generate text in the style of Ali Wong, so as a first step, let's extract the text from her comedy routine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3Mtcqg4aWPK"
      },
      "outputs": [],
      "source": [
        "# Read in the corpus, including punctuation!\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_pickle('corpus.pkl')\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2r0BK12MaWPL"
      },
      "outputs": [],
      "source": [
        "# Extract only Ali Wong's text\n",
        "ali_text = data.transcript.loc['ali']\n",
        "ali_text[:200]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUVX6aF8aWPL"
      },
      "source": [
        "## Build a Markov Chain Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53A7p7sfaWPL"
      },
      "source": [
        "We are going to build a simple Markov chain function that creates a dictionary:\n",
        "* The keys should be all of the words in the corpus\n",
        "* The values should be a list of the words that follow the keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RmKzvtipaWPM"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def markov_chain(text):\n",
        "    '''The input is a string of text and the output will be a dictionary with each word as\n",
        "       a key and each value as the list of words that come after the key in the text.'''\n",
        "    \n",
        "    # Tokenize the text by word, though including punctuation\n",
        "    words = text.split(' ')\n",
        "    \n",
        "    # Initialize a default dictionary to hold all of the words and next words\n",
        "    m_dict = defaultdict(list)\n",
        "    \n",
        "    # Create a zipped list of all of the word pairs and put them in word: list of next words format\n",
        "    for current_word, next_word in zip(words[0:-1], words[1:]):\n",
        "        m_dict[current_word].append(next_word)\n",
        "\n",
        "    # Convert the default dict back into a dictionary\n",
        "    m_dict = dict(m_dict)\n",
        "    return m_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2Qtn2PSaWPM"
      },
      "outputs": [],
      "source": [
        "# Create the dictionary for Ali's routine, take a look at it\n",
        "ali_dict = markov_chain(ali_text)\n",
        "ali_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3IHxpBxaWPM"
      },
      "source": [
        "## Create a Text Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmbLFtloaWPN"
      },
      "source": [
        "We're going to create a function that generates sentences. It will take two things as inputs:\n",
        "* The dictionary you just created\n",
        "* The number of words you want generated\n",
        "\n",
        "Here are some examples of generated sentences:\n",
        "\n",
        ">'Shape right turn– I also takes so that she’s got women all know that snail-trail.'\n",
        "\n",
        ">'Optimum level of early retirement, and be sure all the following Tuesday… because it’s too.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ny_3YhwbaWPU"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_sentence(chain, count=15):\n",
        "    '''Input a dictionary in the format of key = current word, value = list of next words\n",
        "       along with the number of words you would like to see in your generated sentence.'''\n",
        "\n",
        "    # Capitalize the first word\n",
        "    word1 = random.choice(list(chain.keys()))\n",
        "    sentence = word1.capitalize()\n",
        "\n",
        "    # Generate the second word from the value list. Set the new word as the first word. Repeat.\n",
        "    for i in range(count-1):\n",
        "        word2 = random.choice(chain[word1])\n",
        "        word1 = word2\n",
        "        sentence += ' ' + word2\n",
        "\n",
        "    # End it with a period\n",
        "    sentence += '.'\n",
        "    return(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qNbRht8aWPU"
      },
      "outputs": [],
      "source": [
        "generate_sentence(ali_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References:\n",
        "Code forked from adashofdata/nlp-in-python-tutorial"
      ],
      "metadata": {
        "id": "UHCNcFf7swGN"
      }
    }
  ]
}